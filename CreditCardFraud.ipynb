{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "The following dataset is being downloaded from Kaggle. This data set contains credit card data that has some fradulant events. As part of this notebook we will do classification that contains two classes Fraud, No Fraud. We will try various algorithms to findout the best algorithm that we can apply for this kind of datasets.\n",
    "This data set contains the variable V1-V28 which are the principal components of the original dataset. Due to security concerns the original dataset was not released to public. As part of initial data analysis we are gonna use all the variables in our model building.\n",
    "Here we have a Time variable which shows the transaction time each event that happened relative to the first transaction in the dataset.\n",
    "Lets do the exploratory analysis on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix,roc_auc_score,roc_curve,classification_report,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the creditcard csv file located at CreditCardData Folder\n",
    "creditData = pd.read_csv(\"CreditCardData/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List out all the columns of the dataset\n",
    "creditData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis(EDA)\n",
    "Use pandasprofiling to do Exploratory analysis using single line of coding. For more refer\n",
    "https://medium.com/@rameshbattu1989/one-line-code-for-quick-exploratory-data-analysis-eda-63fec70bd65c\n",
    "https://www.datadiscuss.com/data-analysis-with-one-line-of-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas_profiling\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below code will take little time to run. Grab a cup of coffee and wait for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b9594d4c944172a63bde6607b5818a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Summarize dataset'), FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a150a8978ed4233b243481be9a74bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Generate report structure'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397771d40801420d936a32b2f1e3589a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Render HTML'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8d607610be4ac1997fb6826c65855e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Export report to file'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# You can save the profiling report as html page.\n",
    "profile = ProfileReport(creditData, title=\"Pandas Credit Card Data Profiling Report\")\n",
    "profile #Saved as variable \n",
    "profile.to_file(output_file=\"CrediCardEDA.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline view of the pandas profiling report.\n",
    "%matplotlib inline\n",
    "creditData.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List out the data types of the Credit Data.\n",
    "##### We have 30 variables of type float and One variable of type integer.\n",
    "##### The variables Time, V1-V28 and Amount are Independent Variables.\n",
    "##### The out put variable is Class which is having two classes 1(Fraud) and 0(No-Fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      float64\n",
       "V1        float64\n",
       "V2        float64\n",
       "V3        float64\n",
       "V4        float64\n",
       "V5        float64\n",
       "V6        float64\n",
       "V7        float64\n",
       "V8        float64\n",
       "V9        float64\n",
       "V10       float64\n",
       "V11       float64\n",
       "V12       float64\n",
       "V13       float64\n",
       "V14       float64\n",
       "V15       float64\n",
       "V16       float64\n",
       "V17       float64\n",
       "V18       float64\n",
       "V19       float64\n",
       "V20       float64\n",
       "V21       float64\n",
       "V22       float64\n",
       "V23       float64\n",
       "V24       float64\n",
       "V25       float64\n",
       "V26       float64\n",
       "V27       float64\n",
       "V28       float64\n",
       "Amount    float64\n",
       "Class       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of the data set. Total 284807 observations ith 31 variables.\n",
    "creditData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the dataset\n",
    "creditData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in the base credit card data set 284807\n"
     ]
    }
   ],
   "source": [
    "print(\"Total records in the base credit card data set\",len(creditData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequecny of the Fraud(1) and No-Fraud(0) Data Elements\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Frequecny of the Fraud(1) and No-Fraud(0) Data Elements\")\n",
    "creditData['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above there are 492 Fraud events among 284807 events. So we have only 0.17 percentage only.\n",
    "#### It clearly says there is lot of imbalance in the dataset. We have majority of the No-Fraud events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for the duplicate observations in the data set.\n",
    "creditData_duplicate = creditData[creditData.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate rows 1081\n"
     ]
    }
   ],
   "source": [
    "print(\"Total duplicate rows\",len(creditData_duplicate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate elements in the Credit Card Data\n",
    "creditData_noDuplicates = creditData.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows after deleting the duplicates 283726\n"
     ]
    }
   ],
   "source": [
    "print(\"No of rows after deleting the duplicates\",len(creditData_noDuplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the Amount column as it is on different scale and it effects the Model building.\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajeshpriyanka/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# normalise the amount column \n",
    "creditData_noDuplicates['normAmount'] = StandardScaler().fit_transform(np.array(creditData_noDuplicates['Amount']).reshape(-1, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Time and Amount columns as they are not relevant for prediction purpose  \n",
    "creditData_noDuplicates = creditData_noDuplicates.drop(['Time', 'Amount'], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Column that contains the Class column which is the Dependent Variable.\n",
    "y = creditData_noDuplicates['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that has only the independent variables.\n",
    "X = creditData_noDuplicates.drop('Class', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into 70 and 30 ratio\n",
    "from sklearn.model_selection import train_test_split \n",
    "  \n",
    "# split into 70:30 ration \n",
    "creditData_noDuplicates_x_train, creditData_noDuplicates_x_test, creditData_noDuplicates_y_train, creditData_noDuplicates_y_test = train_test_split(X, y, test_size = 0.3, random_state = 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
       "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
       "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditData_noDuplicates_x_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Techniques for Imbalanced Data Sets\n",
    "From the above we see there is a lot of imbalance in the output class. We need to address the above problem.\n",
    "This is called imbalance data set handling. We have three methods to handle the above scenario.\n",
    "1. Oversampling - Increase the low class variable by duplicating the data.\n",
    "2. Undersampling - Decrease majority class samples.\n",
    "3. SMOTE - Create new input of the minority classes using nearest neightbouring method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a creditData_train using the Independent Variables.\n",
    "creditData_train = creditData_noDuplicates_x_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Dependent variable to create the Upsampling data.\n",
    "creditData_train['Class'] =  creditData_noDuplicates_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
       "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
       "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount', 'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditData_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the Fraud and No-Fraud events into two different datasets.\n",
    "credit_card_fraud = creditData_train[creditData_train['Class']==1]\n",
    "credit_card_no_fraud = creditData_train[creditData_train['Class']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fraud rows :  329\n",
      "Number of Non-Fraud rows :  198279\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of fraud rows : \",len(credit_card_fraud))\n",
    "print(\"Number of Non-Fraud rows : \",len(credit_card_no_fraud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the upsample Credit data using the Minority Class(Fraud).\n",
    "credit_card_fraud_upsampled = resample(credit_card_fraud, \n",
    "                                       replace = True,\n",
    "                                      n_samples = len(credit_card_no_fraud),\n",
    "                                      random_state = 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Upsampling the fraud events were equal to No-Fraud events :  198279\n"
     ]
    }
   ],
   "source": [
    "print(\"After Upsampling the fraud events were equal to No-Fraud events : \",len(credit_card_fraud_upsampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the no-fraud and upsample fraud events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the oversample minority class(Fraud) events with original No-Fraud events.\n",
    "creditData_upsampled = pd.concat([credit_card_no_fraud, credit_card_fraud_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After upsampling the number of total events :  396558\n"
     ]
    }
   ],
   "source": [
    "print(\"After upsampling the number of total events : \", len(creditData_upsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Merge check for the Fraud and No-Fraud events.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    198279\n",
       "0    198279\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"After Merge check for the Fraud and No-Fraud events.\")\n",
    "creditData_upsampled['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Down sampling - Down Sampling the non-fraud events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Downsampling data using the Majority class\n",
    "creditData_noFraud_downsample=resample(credit_card_no_fraud,\n",
    "                                      replace=False,\n",
    "                                      n_samples=len(credit_card_fraud),\n",
    "                                      random_state=54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After down sampling the non fraud events :  329\n"
     ]
    }
   ],
   "source": [
    "print(\"After down sampling the non fraud events : \", len(creditData_noFraud_downsample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Fraud and Downsampled No-Fraud events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditData_downsampled = pd.concat([credit_card_fraud, creditData_noFraud_downsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After down sampling the number of total events :  658\n"
     ]
    }
   ],
   "source": [
    "print(\"After down sampling the number of total events : \", len(creditData_downsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Down Sample Merge check for the Fraud and No-Fraud events.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    329\n",
       "0    329\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"After Down Sample Merge check for the Fraud and No-Fraud events.\")\n",
    "creditData_downsampled['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /home/rajeshpriyanka/anaconda3/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /home/rajeshpriyanka/anaconda3/lib/python3.7/site-packages (from imblearn) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/rajeshpriyanka/anaconda3/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.18.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in /home/rajeshpriyanka/anaconda3/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (0.23.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/rajeshpriyanka/anaconda3/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/rajeshpriyanka/anaconda3/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/rajeshpriyanka/anaconda3/lib/python3.7/site-packages (from scikit-learn>=0.23->imbalanced-learn->imblearn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state = 54, \n",
    "           k_neighbors = 5,\n",
    "          sampling_strategy = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the above defined smote to the Independent and Dependent Data Frames which was created above.\n",
    "creditData_noDuplicate_x_train_smote, creditData_noDuplicate_y_train_smote = sm.fit_sample(creditData_noDuplicates_x_train,\n",
    "                                                                                   creditData_noDuplicates_y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the smote X-Train  (396558, 30)\n",
      "The size of the smore y_train (396558,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of the smote X-Train \", creditData_noDuplicate_x_train_smote.shape)\n",
    "print(\"The size of the smore y_train\", creditData_noDuplicate_y_train_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
       "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
       "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount', 'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditData_noDuplicates_x_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Models on the base data without handling Imbalanced Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression, RandomForest and Support Vector Classification\n",
    "#### Here we defined a function that takes the train and test dataset to build the model.\n",
    "#### Here we are not looking into performance tuning and we will deal with that in another chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "def runModels(x_train,y_train,x_test,y_test):\n",
    "    \n",
    "    # Logistic regression which is recommended for classification\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train,y_train)\n",
    "\n",
    "    creditData_noDuplicates_x_train.columns\n",
    "\n",
    "    logistic_creditData_predict = lr.predict(x_test)\n",
    "\n",
    "    accuracy_score(y_test, logistic_creditData_predict)\n",
    "\n",
    "    f1_score(y_test, logistic_creditData_predict)\n",
    "\n",
    "    recall_score(y_test, logistic_creditData_predict)\n",
    "\n",
    "    print(\"Logistic Regression Classification Report : \\n\",classification_report(y_test, logistic_creditData_predict))\n",
    "\n",
    "    # RandomForest which is Tree based Algorithm.\n",
    "    random_forest = RandomForestClassifier()\n",
    "    random_forest.fit(x_train,y_train)\n",
    "    \n",
    "    # Support Vector Classification Algorithm.\n",
    "    svc = SVC()\n",
    "    svc.fit(x_train,y_train)\n",
    "\n",
    "    random_forest_predict = random_forest.predict(x_test)\n",
    "    svc_predict = svc.predict(x_test)\n",
    "\n",
    "    print(\"Random Forest Classification Report : \\n\",\n",
    "          classification_report(y_test, random_forest_predict))\n",
    "\n",
    "    print(\"SVC Classification Report : \\n\",\n",
    "          classification_report(y_test, svc_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below four chunks will take the above defined function for BaseData(With Imbalanced Dataset), \n",
    "Upsampled Data, Downsampled and Smote based data.\n",
    "Grab a cup of coffee may be couple of cups as it will take little time to run the models and fetch the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     84974\n",
      "           1       0.88      0.58      0.70       144\n",
      "\n",
      "    accuracy                           1.00     85118\n",
      "   macro avg       0.94      0.79      0.85     85118\n",
      "weighted avg       1.00      1.00      1.00     85118\n",
      "\n",
      "Random Forest Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     84974\n",
      "           1       0.91      0.76      0.83       144\n",
      "\n",
      "    accuracy                           1.00     85118\n",
      "   macro avg       0.95      0.88      0.91     85118\n",
      "weighted avg       1.00      1.00      1.00     85118\n",
      "\n",
      "SVC Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     84974\n",
      "           1       0.94      0.62      0.74       144\n",
      "\n",
      "    accuracy                           1.00     85118\n",
      "   macro avg       0.97      0.81      0.87     85118\n",
      "weighted avg       1.00      1.00      1.00     85118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Run the models on the base data\n",
    "runModels(creditData_noDuplicates_x_train, creditData_noDuplicates_y_train,\n",
    "          creditData_noDuplicates_x_test, creditData_noDuplicates_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajeshpriyanka/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     84974\n",
      "           1       0.06      0.88      0.11       144\n",
      "\n",
      "    accuracy                           0.98     85118\n",
      "   macro avg       0.53      0.93      0.55     85118\n",
      "weighted avg       1.00      0.98      0.99     85118\n",
      "\n",
      "Random Forest Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     84974\n",
      "           1       0.89      0.76      0.82       144\n",
      "\n",
      "    accuracy                           1.00     85118\n",
      "   macro avg       0.95      0.88      0.91     85118\n",
      "weighted avg       1.00      1.00      1.00     85118\n",
      "\n",
      "SVC Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     84974\n",
      "           1       0.10      0.87      0.19       144\n",
      "\n",
      "    accuracy                           0.99     85118\n",
      "   macro avg       0.55      0.93      0.59     85118\n",
      "weighted avg       1.00      0.99      0.99     85118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Run Models on the upsampled data\n",
    "upsample_y_train = creditData_upsampled.Class\n",
    "upsample_x_train = creditData_upsampled.drop('Class', axis=1)\n",
    "\n",
    "upsample_x_train.shape\n",
    "\n",
    "upsample_y_train.shape\n",
    "\n",
    "runModels(upsample_x_train, upsample_y_train,\n",
    "          creditData_noDuplicates_x_test, creditData_noDuplicates_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     84974\n",
      "           1       0.04      0.90      0.07       144\n",
      "\n",
      "    accuracy                           0.96     85118\n",
      "   macro avg       0.52      0.93      0.52     85118\n",
      "weighted avg       1.00      0.96      0.98     85118\n",
      "\n",
      "Random Forest Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     84974\n",
      "           1       0.05      0.89      0.09       144\n",
      "\n",
      "    accuracy                           0.97     85118\n",
      "   macro avg       0.52      0.93      0.54     85118\n",
      "weighted avg       1.00      0.97      0.98     85118\n",
      "\n",
      "SVC Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     84974\n",
      "           1       0.07      0.88      0.12       144\n",
      "\n",
      "    accuracy                           0.98     85118\n",
      "   macro avg       0.53      0.93      0.56     85118\n",
      "weighted avg       1.00      0.98      0.99     85118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Run Model on the downsampled data\n",
    "downsample_y_train = creditData_downsampled.Class\n",
    "downsample_x_train = creditData_downsampled.drop('Class', axis=1)\n",
    "\n",
    "downsample_x_train.shape\n",
    "\n",
    "downsample_y_train.shape\n",
    "\n",
    "runModels(downsample_x_train, downsample_y_train,\n",
    "          creditData_noDuplicates_x_test, creditData_noDuplicates_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest which is Tree based Algorithm.\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(creditData_noDuplicate_x_train_smote,creditData_noDuplicate_y_train_smote)\n",
    "\n",
    "random_forest_predict = random_forest.predict(creditData_noDuplicates_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  9, 11, 10,  3, 16,  2, 15,  1,  6, 20,  0,  8,  7,  4, 26, 18,\n",
       "       28,  5, 17, 12, 25, 27, 14, 19, 24, 21, 23, 22])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 13 (0.177873) cumilative importance 0.177873\n",
      "2. feature 9 (0.145465) cumilative importance 0.323338\n",
      "3. feature 11 (0.110545) cumilative importance 0.433883\n",
      "4. feature 10 (0.098882) cumilative importance 0.532765\n",
      "5. feature 3 (0.090378) cumilative importance 0.623143\n",
      "6. feature 16 (0.066037) cumilative importance 0.689180\n",
      "7. feature 2 (0.048140) cumilative importance 0.737320\n",
      "8. feature 15 (0.038779) cumilative importance 0.776098\n",
      "9. feature 1 (0.027783) cumilative importance 0.803882\n",
      "10. feature 6 (0.024042) cumilative importance 0.827924\n",
      "11. feature 20 (0.015436) cumilative importance 0.843360\n",
      "12. feature 0 (0.014557) cumilative importance 0.857918\n",
      "13. feature 8 (0.014211) cumilative importance 0.872128\n",
      "14. feature 7 (0.013604) cumilative importance 0.885732\n",
      "15. feature 4 (0.011831) cumilative importance 0.897563\n",
      "16. feature 26 (0.010696) cumilative importance 0.908259\n",
      "17. feature 18 (0.010533) cumilative importance 0.918793\n",
      "18. feature 28 (0.010195) cumilative importance 0.928987\n",
      "19. feature 5 (0.008437) cumilative importance 0.937424\n",
      "20. feature 17 (0.008113) cumilative importance 0.945537\n",
      "21. feature 12 (0.008083) cumilative importance 0.953619\n",
      "22. feature 25 (0.007832) cumilative importance 0.961451\n",
      "23. feature 27 (0.007152) cumilative importance 0.968604\n",
      "24. feature 14 (0.006894) cumilative importance 0.975498\n",
      "25. feature 19 (0.005135) cumilative importance 0.980632\n",
      "26. feature 24 (0.005112) cumilative importance 0.985744\n",
      "27. feature 21 (0.004931) cumilative importance 0.990675\n",
      "28. feature 23 (0.004773) cumilative importance 0.995448\n",
      "29. feature 22 (0.004552) cumilative importance 1.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "importances = random_forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in random_forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "sum = 0\n",
    "for f in range(X.shape[1]):\n",
    "    sum = sum + importances[indices[f]]\n",
    "    print(\"%d. feature %d (%f) cumilative importance %f\" % (f + 1, indices[f], importances[indices[f]], sum))\n",
    "\n",
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajeshpriyanka/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     84974\n",
      "           1       0.05      0.89      0.10       144\n",
      "\n",
      "    accuracy                           0.97     85118\n",
      "   macro avg       0.53      0.93      0.54     85118\n",
      "weighted avg       1.00      0.97      0.99     85118\n",
      "\n",
      "Random Forest Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     84974\n",
      "           1       0.85      0.80      0.82       144\n",
      "\n",
      "    accuracy                           1.00     85118\n",
      "   macro avg       0.93      0.90      0.91     85118\n",
      "weighted avg       1.00      1.00      1.00     85118\n",
      "\n",
      "SVC Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     84974\n",
      "           1       0.10      0.88      0.18       144\n",
      "\n",
      "    accuracy                           0.99     85118\n",
      "   macro avg       0.55      0.93      0.58     85118\n",
      "weighted avg       1.00      0.99      0.99     85118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Run Model on the smote data\n",
    "creditData_noDuplicate_x_train_smote.shape\n",
    "\n",
    "creditData_noDuplicate_y_train_smote.shape\n",
    "\n",
    "runModels(creditData_noDuplicate_x_train_smote, creditData_noDuplicate_y_train_smote,\n",
    "          creditData_noDuplicates_x_test, creditData_noDuplicates_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network model for the classification with K fold validation\n",
    "\n",
    "# multi-class classification with Keras\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_neural = creditData_noDuplicate_x_train_smote.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_neural = np.asarray(x_neural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.29839762,  0.73029468,  0.09805872, ...,  0.44071875,\n",
       "         0.27390478, -0.17984372],\n",
       "       [ 2.13813813, -0.70091711, -1.42902586, ..., -0.05579137,\n",
       "        -0.0517516 , -0.19274313],\n",
       "       [ 0.97126042, -0.77037646, -0.3872148 , ..., -0.02957813,\n",
       "         0.01331461,  0.2477139 ],\n",
       "       ...,\n",
       "       [ 0.20501152,  2.16574035, -2.40024073, ..., -0.35207314,\n",
       "        -0.27774777, -0.34933322],\n",
       "       [-1.53574182,  0.8400378 , -1.55329971, ...,  0.10914303,\n",
       "        -0.20067219, -0.29427853],\n",
       "       [-3.81460368,  2.79609072, -3.45455497, ..., -1.34898009,\n",
       "        -1.00410962, -0.07648328]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396558, 29)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditData_noDuplicate_x_train_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditData_noDuplicate_y_train_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(creditData_noDuplicate_y_train_smote)\n",
    "encoded_Y = encoder.transform(creditData_noDuplicate_y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    " \n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential([\n",
    "    Dense(units=16,input_dim = 29,activation='relu'),\n",
    "    Dense(units=24,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units=20,activation='relu'),\n",
    "    Dense(units=24,activation='relu'),\n",
    "    Dense(1,activation='sigmoid')])\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "build_fn = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 16)                480       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 20)                500       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 24)                504       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 1,917\n",
      "Trainable params: 1,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_fn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "26438/26438 [==============================] - 48s 2ms/step - loss: 0.0334 - accuracy: 0.9886\n",
      "Epoch 2/5\n",
      "26438/26438 [==============================] - 50s 2ms/step - loss: 0.0116 - accuracy: 0.9969\n",
      "Epoch 3/5\n",
      "26438/26438 [==============================] - 50s 2ms/step - loss: 0.0087 - accuracy: 0.9978\n",
      "Epoch 4/5\n",
      "26438/26438 [==============================] - 49s 2ms/step - loss: 0.0075 - accuracy: 0.9982\n",
      "Epoch 5/5\n",
      "26438/26438 [==============================] - 52s 2ms/step - loss: 0.0068 - accuracy: 0.9984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdd60389f90>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_fn.fit(creditData_noDuplicate_x_train_smote,creditData_noDuplicate_y_train_smote,batch_size=15,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_fn_predict = build_fn.predict(creditData_noDuplicates_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_copy = pd.DataFrame(creditData_noDuplicates_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = build_fn_predict.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198279"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(creditData_noDuplicate_y_train_smote[creditData_noDuplicate_y_train_smote == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0  84974  0\n",
       "1    144  0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(creditData_noDuplicates_y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2660/2660 [==============================] - 3s 1ms/step - loss: 0.0200 - accuracy: 0.9983\n"
     ]
    }
   ],
   "source": [
    "score = build_fn.evaluate(creditData_noDuplicates_x_test,creditData_noDuplicates_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(creditData_noDuplicate_y_train_smote[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-131e248b4a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreditData_noDuplicates_y_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbuild_fn_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1927\u001b[0m     \"\"\"\n\u001b[1;32m   1928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 91\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "print(classification_report(creditData_noDuplicates_y_test,build_fn_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, x_neural, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the Classification report of all the above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
